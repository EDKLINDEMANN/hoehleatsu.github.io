---
layout: post
title: "The Proof-Calculation Ping Pong"
tags: [rstats, stats, quality]
bibliography: ~/Literature/Bibtex/jabref.bib
comments: true
---

```{r,include=FALSE,echo=FALSE,message=FALSE}
##If default fig.path, then set it.
if (knitr::opts_chunk$get("fig.path") == "figure/") {
  knitr::opts_knit$set( base.dir = '/Users/hoehle/Sandbox/Blog/')
  knitr::opts_chunk$set(fig.path="figure/source/2017-09-01-proofpingpong/")
}
fullFigPath <- paste0(knitr::opts_knit$get("base.dir"),knitr::opts_chunk$get("fig.path"))
filePath <- file.path("","Users","hoehle","Sandbox", "Blog", "figure", "source", "2017-09-01-proofpingpong")

knitr::opts_chunk$set(echo = TRUE,fig.width=8,fig.height=4,fig.cap='',fig.align='center',echo=FALSE,dpi=72*2) # autodep=TRUE
options(width=90)

suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(kableExtra))
options(knitr.table.format = "html")
theme_set(theme_bw())
```


## Abstract

The proof-calculation ping-pong is the process of iteratively
improving a statistical analysis by comparing results from two
independent analysis approaches until agreement. We use the `daff`
package to simplify the comparison of the two results.


<center>
```{r,results='asis',echo=FALSE,fig.cap="Daffodil"}
cat(paste0("![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/A_Perfect_Pair_Daffodills_%28Narcissus%29_-_8.jpg/320px-A_Perfect_Pair_Daffodills_%28Narcissus%29_-_8.jpg)"))
```
</center>

{% include license.html %}

## Introduction

If you are a statistician working in climate science, data driven
journalism, official statistics, public health, economics or any
related field working with *real* data, chances are that you have to
perform analyses, where you know there is a zero tolerance for errors.
The easiest way to ensure the correctness of such an analysis is to
check your results over and over again (the **iterated 2-eye
principle**). A better approach is to pair-program the analysis by
either having a colleague read through your code (the **4-eye
principle**) or have a skilled colleague completely redo your analysis
from scratch using her favorite toolchain (the **2-mindset
principle**). Structured software development in the form of,
e.g. version control and unit tests, provides valuable inspiration on
how to ensure the quality of your code. However, when it comes to
pair-programming analyses, surprisingly many steps remain manual. The
`daff` package provides the equivalent of a `diff` statement on data
frames and we shall illustrate its use by automatizing the comparison
step of the statistical proof-calculation ping-pong.

## Case Story

Ada and Bob have to proof-calculate the quadrennial official
statistics on the total income and number of employed people in
[fitness centers](https://www.destatis.de/DE/Publikationen/Qualitaetsberichte/Dienstleistungen/SonstDienstleistungsbereiche2010.pdf?__blob=publicationFile)
in their country. A sample of fitness centers is asked to fill out a
questionnaire containing their yearly sales volume, staff costs and number
of employees. For this post we will ignore the complex survey part of
the problem and just pretend that our sample corresponds to the
population (complete inventory count). After the questionnaire phase,
the following data are available to Ada and Bob.

```{r,message=FALSE}
fitness <- readr::read_csv(file.path(filePath,"fitness.csv"))
kable(fitness) %>% kable_styling(bootstrap_options = c("striped", "hover"))
```

Here `Id` denotes the unique identifier of the sampled fitness center,
`Version` indicates the version of a center's questionnaire and
`Region` denotes the region in which the center is located. In case a
center's questionnaire lacks information or has inconsistent
information, the protocol is to get back to the center and have it
send a revised questionnaire. All Ada and Bob now need to do is
aggregate the data per region in order to obtain region stratified
estimates of:

* the overall number of fitness centres
* total sales volume
* total staff cost
* total number of people employed in fitness centres

However, the analysis protocol instructs that only fitness centers
with an annual sales volume larger than or equal to â‚¬17,500 are to be
included in the analysis. Furthermore, if missing values occur, they
are to be ignored in the summations. Since a lot of muscle will be
angered in case of errors, Ada and Bob agree on following the 2-mindset
procedure and meet after an hour to discuss their results. Here is what
each of them came up with.

### Ada

Ada loves the tidyverse and in particular `dplyr`. This is her solution:

```{r, echo=TRUE}
ada <- fitness %>% na.omit() %>% group_by(Region,Id) %>% top_n(1,Version) %>%
  group_by(Region) %>%
  filter(`Sales Volume` >= 17500) %>%
  summarise(`NoOfUnits`=n(),
            `Sales Volume`=sum(`Sales Volume`),
            `Staff Costs`=sum(`Staff Costs`),
            People=sum(People))
ada
```

### Bob

Bob has a dark past as database developer and only since recently experiences
the joys of R. He therefore chooses a no-SQL-server-necessary [`SQLite` within
R](https://www.r-bloggers.com/r-and-sqlite-part-1/) approach:

```{r, echo=TRUE}
library(RSQLite)
## Create ad-hoc database
db <- dbConnect(SQLite(), dbname = file.path(filePath,"Test.sqlite"))
##Move fitness data into the ad-hoc DB
dbWriteTable(conn = db, name = "FITNESS", fitness, overwrite=TRUE, row.names=FALSE)
##Query using SQL
bob <- dbGetQuery(db, "
    SELECT Region,
           COUNT(*) As NoOfUnits,
           SUM([Sales Volume]) As [Sales Volume],
           SUM([Staff Costs]) AS [Staff Costs],
           SUM(People) AS People
    FROM FITNESS
    WHERE [Sales Volume] > 17500 GROUP BY Region
")
bob
```

### The Ping-Pong phase

After Ada and Bob each have a result, they compare their
resulting `data.frames`s using the
[`daff`](https://cran.r-project.org/web/packages/daff/index.html)
package, which was recently presented by [\@edwindjonge](https://twitter.com/edwindjonge) at the [useR in Brussels](https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/Daff-diff-patch-and-merge-for-dataframes).


```{r, echo=TRUE}
library(daff)
diff <- diff_data(ada, bob)
print.data_diff <- function(x) x$get_data() %>% filter(`@@` != "")
print(diff)
```

*Note*: Currently, `daff` has the semi-annoying feature of not being
able to show all the diffs when printing, but just `n` lines of the
head and tail. As a consequence, for the purpose of this post, we
overwrite the printing function such that it always shows all rows with
differences.

After Ada's and Bob's serve, the two realize that their results just
agree for one region ('E'). They therefore decide to first focus on
agreeing on the number of units per region.

```{r}
diff$get_data() %>% filter(`@@` != "") %>% select(`@@`, Region, NoOfUnits)
```

One obvious reason for the discrepancies appears to be that Bob has
results for an extra `<NA>` region. Therefore, Bob takes another look at his
management of missing values and decided to improve his code by:

#### Pong Bob
```{r, echo=TRUE}
bob2 <- dbGetQuery(db, "
    SELECT Region,
           COUNT(*) As NoOfUnits,
           SUM([Sales Volume]) As [Sales Volume],
           SUM([Staff Costs]) AS [Staff Costs],
           SUM(People) AS People
    FROM FITNESS
    WHERE ([Sales Volume] > 17500 AND REGION IS NOT NULL)
    GROUP BY Region
")
diff2 <- diff_data(ada, bob2, ordered=FALSE,count_like_a_spreadsheet=FALSE)
diff2 %>% print()
```

#### Ping Bob

Better. Now the `NA` region is gone, but still quite some differences
remain. *Note*: You may at this point want to stop reading and try
yourself to fix the analysis (data and code are available from the
[github repo]()).

#### Pong

Here is how the story continues. Bob notices that he forgot to handle
possible duplicate records and massages the SQL query to improve on this.

```{r}
#https://stackoverflow.com/questions/14802507/find-first-row-from-join-or-group
```
```{r, echo=TRUE}
bob3 <- dbGetQuery(db, "
    SELECT Region,
           COUNT(*) As NoOfUnits,
           SUM([Sales Volume]) As [Sales Volume],
           SUM([Staff Costs]) AS [Staff Costs],
           SUM(People) AS People FROM
    (SELECT Id, MAX(Version), Region, [Sales Volume], [Staff Costs], People FROM FITNESS GROUP BY Id)
    WHERE ([Sales Volume] >= 17500 AND REGION IS NOT NULL)
    GROUP BY Region
")
diff3 <- diff_data(ada, bob3, ordered=FALSE,count_like_a_spreadsheet=FALSE)
diff3 %>% print()
```

#### Ping
Looking at this code and comparing with Ada, Bob is sort of envious
that she was able to just use the `group_by` and `top_n` function.
However, `daff` shows that there still is one difference left. By
looking more carefully at Ada's code it becomes clear that she
accidentally (?) leaves out one unit in D. The reason is her too
liberate use of `na.omit`, which also removes the one entry with an
`NA` in just one of the not so important columns. Here is her modified code:

```{r, echo=TRUE}
ada2 <- fitness %>% filter(!is.na(Region)) %>% group_by(Region,Id) %>% top_n(1,Version) %>%
  group_by(Region) %>%
  filter(`Sales Volume` >= 17500) %>%
  summarise(`NoOfUnits`=n(),
            `Sales Volume`=sum(`Sales Volume`),
            `Staff Costs`=sum(`Staff Costs`),
            People=sum(People))
(diff_final <- diff_data(ada2,bob3)) %>% print()
```

#### Ping
Oops, forgot to take care of the `NA` in the summation:

```{r,echo=TRUE}
ada3 <- fitness %>% filter(!is.na(Region)) %>% group_by(Region,Id) %>% top_n(1,Version) %>%
  group_by(Region) %>%
  filter(`Sales Volume` >= 17500) %>%
  summarise(`NoOfUnits`=n(),
            `Sales Volume`=sum(`Sales Volume`),
            `Staff Costs`=sum(`Staff Costs`),
            People=sum(People,na.rm=TRUE))
(diff_final <- diff_data(ada3,bob3)) %>% print()
```

## Conclusion

Finally, their results agree and they move on to production. Their
results are published in a
[nice report](https://www.destatis.de/DE/Publikationen/Thematisch/DienstleistungenFinanzdienstleistungen/KostenStruktur/KostenstrukturFitness2020163109004.pdf?__blob=publicationFile)?

[**Question 1:**] Do you agree with their results?

```{r,echo=TRUE}
ada3
```

As shown, the ping-pong game is quite manual and particularly
annoying, if at some point someone steps into the office with a
statement like 'Btw, I found some extra questionnaires, which need to
be added to the analysis". However, the two now aligned analysis
scripts an the corresponding daff-overlaying could be put into a
script, which is triggered every time the data change. In case new
discrepancies emerge as `nrow(diff$get_data()) > 0`, the two could
then be automatically informed.

[**Question 2:**]: Are you aware of any other good ways and tools to structure and automatize such a process? If so, please share your experiences as a Disqus comment below.
